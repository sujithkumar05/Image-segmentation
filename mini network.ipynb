{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'\n",
    "IMAGE_HEIGHT = 320\n",
    "IMAGE_WIDTH = 320\n",
    "BATCH_SIZE = 8\n",
    "ENCODER = 'efficientnet-b7'\n",
    "WEIGHTS = 'imagenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masks</th>\n",
       "      <th>images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>white masked images\\95.png</td>\n",
       "      <td>training samples\\95.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>white masked images\\96.png</td>\n",
       "      <td>training samples\\96.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>white masked images\\97.png</td>\n",
       "      <td>training samples\\97.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>white masked images\\98.png</td>\n",
       "      <td>training samples\\98.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>white masked images\\99.png</td>\n",
       "      <td>training samples\\99.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          masks                   images\n",
       "138  white masked images\\95.png  training samples\\95.jpg\n",
       "139  white masked images\\96.png  training samples\\96.jpg\n",
       "140  white masked images\\97.png  training samples\\97.jpg\n",
       "141  white masked images\\98.png  training samples\\98.jpg\n",
       "142  white masked images\\99.png  training samples\\99.jpg"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = pd.read_csv('train.csv')\n",
    "data_frame.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143 entries, 0 to 142\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   masks   143 non-null    object\n",
      " 1   images  143 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data_frame.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_rgb(mask, label=False):\n",
    "    color_map={\n",
    "        0: (0, 0, 0), # Background\n",
    "        1: (255, 0, 0), # Class 1\n",
    "        2: (0, 255, 0), # Class 2\n",
    "        3: (0, 0, 255), # Class 3\n",
    "        4: (0, 128, 128), # Class 4\n",
    "        5: (128, 0, 128), # Class 5\n",
    "    }\n",
    "    color_to_class = {\n",
    "      (255, 0, 0): \"Horse\",  # Red pixel represents Horse\n",
    "      (0, 255, 0): \"Bench\",  # Green pixel represents Bench\n",
    "      (0, 0, 255): \"Water Dispenser\",  # Blue pixel represents Water dispenser\n",
    "      (0, 128, 128): \"Trash bin\",  # Blue pixel represents Dust bin\n",
    "      (128, 0, 128): \"Stop Sign\",  # Blue pixel represents stop sign\n",
    "    }\n",
    "    if isinstance(mask, torch.Tensor):\n",
    "        mask = mask.detach().cpu().numpy()\n",
    "    \n",
    "    # Count the number of occurrences of each RGB color in the predicted mask.\n",
    "    counts = {}\n",
    "    for row in range(320):\n",
    "        for column in range(320):\n",
    "            class_index = mask[0][row][column]\n",
    "            if class_index != 0:\n",
    "                rgb = color_map[class_index]\n",
    "                if rgb in counts:\n",
    "                    counts[rgb] += 1\n",
    "                else:\n",
    "                    counts[rgb] = 1\n",
    "                    \n",
    "    # Determine the RGB color with the highest count that is not the background color.\n",
    "    max_count = 0\n",
    "    max_rgb = None\n",
    "    for rgb, count in counts.items():\n",
    "        if count > max_count and rgb != (0,0,0):\n",
    "            max_count = count\n",
    "            max_rgb = rgb\n",
    "    \n",
    "    # Replace all non-background class indexes in the mask with the chosen RGB color.\n",
    "    if max_rgb is not None:\n",
    "        rgb_mask = np.zeros((320, 320, 3), dtype=np.uint8)\n",
    "        for row in range(320):\n",
    "            for column in range(320):\n",
    "                class_index = mask[0][row][column]\n",
    "                if class_index != 0:\n",
    "                    rgb = color_map[class_index]\n",
    "                    if rgb == max_rgb:\n",
    "                        rgb_mask[row][column] = rgb\n",
    "                    else:\n",
    "                        rgb_mask[row][column] = max_rgb\n",
    "                else:\n",
    "                    rgb_mask[row][column] = (0,0,0)\n",
    "    else:\n",
    "        # If all non-background colors have zero occurrences, use the original function.\n",
    "        rgb_mask = np.zeros((320, 320, 3), dtype=np.uint8)\n",
    "        for row in range(320):\n",
    "            for column in range(320):\n",
    "                class_index = mask[0][row][column]\n",
    "                rgb_mask[row][column] = color_map[class_index]\n",
    "  \n",
    "    if label:\n",
    "        return rgb_mask, color_to_class.get(max_rgb)    \n",
    "    return rgb_mask\n",
    "\n",
    "\n",
    "def show_image(image, mask, pred_image=None):\n",
    "    image = image.permute(1, 2, 0).squeeze().numpy()\n",
    "    mask = mask_to_rgb(mask)\n",
    "\n",
    "    if pred_image is not None:\n",
    "        pred_image = mask_to_rgb(pred_image)\n",
    "\n",
    "    image = image / np.max(image)\n",
    "    mask = mask / np.max(mask)\n",
    "    \n",
    "    if pred_image is not None:\n",
    "        pred_image = pred_image / np.max(pred_image)\n",
    "\n",
    "    if pred_image is None:\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        ax1.set_title('IMAGE')\n",
    "        ax1.imshow(image, cmap='gray')\n",
    "\n",
    "        ax2.set_title('GROUND TRUTH')\n",
    "        ax2.imshow(mask)\n",
    "\n",
    "    else:\n",
    "        f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 5))\n",
    "\n",
    "        ax1.set_title('IMAGE')\n",
    "        ax1.imshow(image, cmap='gray')\n",
    "\n",
    "        ax2.set_title('GROUND TRUTH')\n",
    "        ax2.imshow(mask)\n",
    "\n",
    "        ax3.set_title('MODEL OUTPUT')\n",
    "        ax3.imshow(pred_image)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "def classIndexMask(mask):\n",
    "    # Define a dictionary mapping RGB color values to class indices\n",
    "    color_to_class = {\n",
    "      (0, 0, 0): 0,  # Black pixel represents background\n",
    "      (255, 0, 0): 1,  # Red pixel represents Horse\n",
    "      (0, 255, 0): 2,  # Green pixel represents Bench\n",
    "      (0, 0, 255): 3,  # Blue pixel represents Water dispenser\n",
    "      (0, 128, 128): 4,  # light blue pixel represents Dust bin\n",
    "      (128, 0, 128): 5,  # pink pixel represents stop sign\n",
    "    }\n",
    "\n",
    "    # Load a masked image in RGB format\n",
    "    mask_image = Image.open(mask)\n",
    "    mask_image = mask_image.resize((320, 320))\n",
    "\n",
    "    # Convert the RGB image to a numpy array\n",
    "    mask_array = np.array(mask_image)\n",
    "\n",
    "    # Create a new numpy array to hold the class index mask\n",
    "    class_indices = np.zeros((mask_array.shape[0], mask_array.shape[1]), dtype=np.uint8)\n",
    "\n",
    "    # Iterate over each RGB color value and map it to a class index\n",
    "    for color, class_idx in color_to_class.items():\n",
    "      # Find the indices in the numpy array where the RGB color value matches the dictionary key\n",
    "      color_indices = np.where(np.all(mask_array == color, axis=-1))\n",
    "      # Assign the corresponding class index to the pixels at those indices\n",
    "      class_indices[color_indices] = class_idx\n",
    "    return class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = train_test_split(data_frame, test_size=0.2, random_state=42)\n",
    "\n",
    "validation_data, test_data = train_test_split(testing_data, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \n",
    "        self.df=df\n",
    "        self.transforms = transforms.Resize((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self ,idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = row['images']\n",
    "        mask_path = row['masks']\n",
    "        image_path = dir+row.images.replace('\\\\', '/')\n",
    "        mask_path = dir+row.masks.replace('\\\\', '/')\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = classIndexMask(mask_path)\n",
    "        mask =np.expand_dims(mask ,axis=-1)#(h,w,1)\n",
    "        #(h,w,c) -> (c,h,w)\n",
    "        image = np.transpose(image ,(2,0,1)).astype(np.float32)\n",
    "        mask = np.transpose(mask ,(2,0,1)).astype(np.float32)\n",
    "        original_image = self.transforms(torch.Tensor(image))\n",
    "        original_image = original_image/255.0 # normalizing original image tensor [0,1] range\n",
    "        return original_image, torch.Tensor(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set = SegmentationDataset(training_data)\n",
    "valid_data_set = SegmentationDataset(validation_data)\n",
    "test_data_set = SegmentationDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training dataset: 114\n",
      "Size of Testing dataset: 15\n",
      "Size of Validation dataset: 14\n"
     ]
    }
   ],
   "source": [
    "print('Size of Training dataset: {}'.format(train_data_set.__len__()))\n",
    "print('Size of Testing dataset: {}'.format(test_data_set.__len__()))\n",
    "print('Size of Validation dataset: {}'.format(valid_data_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data_set, batch_size=8, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(valid_data_set, batch_size=8, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_data_set, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches in train data loader: 15\n",
      "Total number of batches in test data loader: 2\n",
      "Total number of batches in validation data loader: 2\n"
     ]
    }
   ],
   "source": [
    "print('Total number of batches in train data loader: {}'.format(len(train_loader)))\n",
    "print('Total number of batches in test data loader: {}'.format(len(test_loader)))\n",
    "print('Total number of batches in validation data loader: {}'.format(len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'builtin_function_or_method' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-3c88e4828040>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34mf'One batch image shape: {image.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34mf'One batch image shape: {mask.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-f49592eb1df7>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'images'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mmask_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'masks'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mimage_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mmask_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'builtin_function_or_method' and 'str'"
     ]
    }
   ],
   "source": [
    "for image , mask in train_loader:\n",
    "    break\n",
    "print (f'One batch image shape: {image.shape}')\n",
    "print (f'One batch image shape: {mask.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'conda' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge opencv=3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
